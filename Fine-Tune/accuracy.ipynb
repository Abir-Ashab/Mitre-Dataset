{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-06T05:53:18.030318Z",
     "iopub.status.busy": "2026-01-06T05:53:18.030036Z",
     "iopub.status.idle": "2026-01-06T05:53:18.256937Z",
     "shell.execute_reply": "2026-01-06T05:53:18.255985Z",
     "shell.execute_reply.started": "2026-01-06T05:53:18.030285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:53:18.259354Z",
     "iopub.status.busy": "2026-01-06T05:53:18.259099Z",
     "iopub.status.idle": "2026-01-06T05:53:22.969132Z",
     "shell.execute_reply": "2026-01-06T05:53:22.968119Z",
     "shell.execute_reply.started": "2026-01-06T05:53:18.259326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q transformers datasets accelerate peft tqdm pandas numpy scikit-learn matplotlib seaborn\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:53:22.970594Z",
     "iopub.status.busy": "2026-01-06T05:53:22.970361Z",
     "iopub.status.idle": "2026-01-06T05:53:22.982522Z",
     "shell.execute_reply": "2026-01-06T05:53:22.981726Z",
     "shell.execute_reply.started": "2026-01-06T05:53:22.970566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Kaggle paths - UPDATE THESE based on your uploaded dataset names\n",
    "MODEL_PATH = \"/kaggle/input/mitre-fine-tuned-model\"  # Your uploaded model dataset\n",
    "DATA_PATH = \"/kaggle/input/mitre-datset\"  # Your test data dataset\n",
    "TEST_FILE = f\"{DATA_PATH}/test.jsonl\"\n",
    "\n",
    "# Evaluation settings\n",
    "EVAL_LIMIT = None  # Set to None for full evaluation, or a number like 100 for quick test\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Test data: {TEST_FILE}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model found: {len(os.listdir(MODEL_PATH))} files\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {MODEL_PATH}\")\n",
    "    print(\"   Please upload your fine_tuned_model as a Kaggle dataset\")\n",
    "\n",
    "if os.path.exists(TEST_FILE):\n",
    "    print(f\"‚úÖ Test file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Test file not found at {TEST_FILE}\")\n",
    "    print(\"   Please add your test dataset to Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:53:22.983958Z",
     "iopub.status.busy": "2026-01-06T05:53:22.983474Z",
     "iopub.status.idle": "2026-01-06T05:53:56.862446Z",
     "shell.execute_reply": "2026-01-06T05:53:56.861348Z",
     "shell.execute_reply.started": "2026-01-06T05:53:22.983937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "print(\"üîÑ Loading fine-tuned model...\\n\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"üìä Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-06T05:53:56.863194Z",
     "iopub.status.idle": "2026-01-06T05:53:56.863436Z",
     "shell.execute_reply": "2026-01-06T05:53:56.863334Z",
     "shell.execute_reply.started": "2026-01-06T05:53:56.863318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"üîÑ Loading test dataset...\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset('json', data_files={'test': TEST_FILE})['test']\n",
    "\n",
    "print(f\"‚úÖ Test dataset loaded: {len(test_dataset):,} examples\")\n",
    "print(f\"\\nüìã Dataset columns: {test_dataset.column_names}\")\n",
    "print(f\"\\nüìã Sample test entry:\")\n",
    "print(f\"   Instruction: {test_dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"   Input: {test_dataset[0]['input'][:100]}...\")\n",
    "print(f\"   Output: {test_dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-06T05:53:56.864823Z",
     "iopub.status.idle": "2026-01-06T05:53:56.865191Z",
     "shell.execute_reply": "2026-01-06T05:53:56.865030Z",
     "shell.execute_reply.started": "2026-01-06T05:53:56.864997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation functions\n",
    "print(\"üîÑ Defining evaluation functions...\\n\")\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, input_text, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given instruction and input.\"\"\"\n",
    "    prompt = f\"\"\"{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (remove the prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "def calculate_exact_match(pred, target):\n",
    "    \"\"\"Calculate exact match accuracy.\"\"\"\n",
    "    return 1.0 if pred.strip().lower() == target.strip().lower() else 0.0\n",
    "\n",
    "def calculate_partial_match(pred, target):\n",
    "    \"\"\"Calculate partial match (keyword overlap).\"\"\"\n",
    "    pred_lower = pred.strip().lower()\n",
    "    target_lower = target.strip().lower()\n",
    "    \n",
    "    # Check if major keywords from target appear in prediction\n",
    "    target_words = set(target_lower.split())\n",
    "    pred_words = set(pred_lower.split())\n",
    "    \n",
    "    if len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(target_words.intersection(pred_words))\n",
    "    return overlap / len(target_words)\n",
    "\n",
    "def calculate_f1_score(pred, target):\n",
    "    \"\"\"Calculate F1 score based on word overlap.\"\"\"\n",
    "    pred_words = set(pred.strip().lower().split())\n",
    "    target_words = set(target.strip().lower().split())\n",
    "    \n",
    "    if len(pred_words) == 0 or len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(pred_words.intersection(target_words))\n",
    "    \n",
    "    precision = overlap / len(pred_words) if len(pred_words) > 0 else 0.0\n",
    "    recall = overlap / len(target_words) if len(target_words) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-06T05:53:56.866781Z",
     "iopub.status.idle": "2026-01-06T05:53:56.867115Z",
     "shell.execute_reply": "2026-01-06T05:53:56.866970Z",
     "shell.execute_reply.started": "2026-01-06T05:53:56.866943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"üöÄ Running evaluation on test set...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Determine sample size\n",
    "if EVAL_LIMIT is None:\n",
    "    eval_samples = test_dataset\n",
    "    print(f\"Evaluating on FULL test set: {len(eval_samples):,} examples\")\n",
    "else:\n",
    "    eval_samples = test_dataset.select(range(min(EVAL_LIMIT, len(test_dataset))))\n",
    "    print(f\"Evaluating on LIMITED test set: {len(eval_samples):,} examples (out of {len(test_dataset):,})\")\n",
    "\n",
    "print(f\"This may take a while...\\n\")\n",
    "\n",
    "results = []\n",
    "exact_matches = 0\n",
    "partial_match_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_samples, desc=\"Evaluating\")):\n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        example['instruction'],\n",
    "        example['input'],\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = calculate_exact_match(prediction, example['output'])\n",
    "    partial_match = calculate_partial_match(prediction, example['output'])\n",
    "    f1 = calculate_f1_score(prediction, example['output'])\n",
    "    \n",
    "    exact_matches += exact_match\n",
    "    partial_match_scores.append(partial_match)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'instruction': example['instruction'],\n",
    "        'input': example['input'],\n",
    "        'expected': example['output'],\n",
    "        'predicted': prediction,\n",
    "        'exact_match': exact_match,\n",
    "        'partial_match': partial_match,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    # Show first few examples\n",
    "    if i < 5:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Instruction: {example['instruction'][:80]}...\")\n",
    "        print(f\"Input: {example['input'][:100]}...\")\n",
    "        print(f\"Expected: {example['output'][:100]}...\")\n",
    "        print(f\"Predicted: {prediction[:100]}...\")\n",
    "        print(f\"Metrics: Exact={exact_match}, Partial={partial_match:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Evaluation completed in {elapsed/60:.2f} minutes ({elapsed/len(eval_samples):.2f} sec/example)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"üìä Dataset Information:\")\n",
    "print(f\"   Total samples evaluated: {len(eval_samples):,}\")\n",
    "print(f\"   Unique labels: {len(unique_labels)}\")\n",
    "print(f\"   Evaluation time: {elapsed/60:.2f} minutes\")\n",
    "print(f\"   Time per sample: {elapsed/len(eval_samples):.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüéØ Key Performance Metrics:\")\n",
    "print(f\"   ‚úì Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   ‚úì Weighted Precision: {precision_weighted:.4f}\")\n",
    "print(f\"   ‚úì Weighted Recall: {recall_weighted:.4f}\")\n",
    "print(f\"   ‚úì Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   ‚úì {output_file}\")\n",
    "print(f\"   ‚úì {metrics_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ Evaluation Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "print(\"üíæ Saving detailed results...\\n\")\n",
    "\n",
    "# Create detailed results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['true_label'] = y_true\n",
    "results_df['predicted_label'] = y_pred\n",
    "results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'evaluation_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Detailed results saved to: {output_file}\")\n",
    "\n",
    "# Create metrics summary\n",
    "metrics_summary = {\n",
    "    'Metric': [\n",
    "        'Accuracy',\n",
    "        'Precision (Macro)',\n",
    "        'Precision (Weighted)',\n",
    "        'Recall (Macro)',\n",
    "        'Recall (Weighted)',\n",
    "        'F1-Score (Macro)',\n",
    "        'F1-Score (Weighted)',\n",
    "        'Exact Match Accuracy',\n",
    "        'Avg Partial Match',\n",
    "        'Avg F1 (Word-level)'\n",
    "    ],\n",
    "    'Score': [\n",
    "        accuracy,\n",
    "        precision_macro,\n",
    "        precision_weighted,\n",
    "        recall_macro,\n",
    "        recall_weighted,\n",
    "        f1_macro,\n",
    "        f1_weighted,\n",
    "        exact_match_accuracy,\n",
    "        avg_partial_match,\n",
    "        avg_f1_word\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "metrics_file = 'metrics_summary.csv'\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"‚úÖ Metrics summary saved to: {metrics_file}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(\"\\nüìã Sample Results (First 10):\")\n",
    "display_cols = ['instruction', 'true_label', 'predicted_label', 'correct', 'f1_score']\n",
    "print(results_df[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìä Correct Predictions: {results_df['correct'].sum()} / {len(results_df)} ({accuracy*100:.2f}%)\")\n",
    "print(f\"üìä Incorrect Predictions: {(~results_df['correct']).sum()} / {len(results_df)} ({(1-accuracy)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Metrics Comparison\n",
    "print(\"üé® Creating metrics visualization...\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create metrics comparison bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Overall Metrics\n",
    "metrics_names = ['Accuracy', 'Precision\\n(Weighted)', 'Recall\\n(Weighted)', 'F1-Score\\n(Weighted)']\n",
    "metrics_values = [accuracy, precision_weighted, recall_weighted, f1_weighted]\n",
    "\n",
    "bars1 = ax1.bar(metrics_names, metrics_values, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'], alpha=0.8)\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3, label='50% baseline')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}\\n({height*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Macro vs Weighted Metrics\n",
    "metrics_comparison = {\n",
    "    'Precision': [precision_macro, precision_weighted],\n",
    "    'Recall': [recall_macro, recall_weighted],\n",
    "    'F1-Score': [f1_macro, f1_weighted]\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars2_1 = ax2.bar(x - width/2, [v[0] for v in metrics_comparison.values()], \n",
    "                   width, label='Macro', color='#3498db', alpha=0.8)\n",
    "bars2_2 = ax2.bar(x + width/2, [v[1] for v in metrics_comparison.values()], \n",
    "                   width, label='Weighted', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro vs Weighted Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics_comparison.keys())\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars2_1, bars2_2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Metrics visualization created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Report\n",
    "print(\"üìä DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, labels=unique_labels, zero_division=0, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Display full report\n",
    "print(classification_report(y_true, y_pred, labels=unique_labels, zero_division=0))\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "print(\"\\nüìà Per-Class Metrics Summary:\")\n",
    "print(report_df.round(4))\n",
    "\n",
    "# Show best and worst performing classes\n",
    "if len(unique_labels) > 5:\n",
    "    print(\"\\nüèÜ TOP 5 BEST PERFORMING CLASSES (by F1-score):\")\n",
    "    class_metrics = report_df[report_df.index.str.startswith('T')].sort_values('f1-score', ascending=False)\n",
    "    print(class_metrics.head(5)[['precision', 'recall', 'f1-score', 'support']].round(4))\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è TOP 5 WORST PERFORMING CLASSES (by F1-score):\")\n",
    "    print(class_metrics.tail(5)[['precision', 'recall', 'f1-score', 'support']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "print(\"üé® Creating confusion matrix visualization...\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(max(12, len(unique_labels)), max(10, len(unique_labels))))\n",
    "\n",
    "# If too many labels, show a subset or use different visualization\n",
    "if len(unique_labels) > 20:\n",
    "    print(f\"‚ö†Ô∏è Large number of labels ({len(unique_labels)}). Showing top 20 most frequent...\")\n",
    "    \n",
    "    # Get top N most frequent labels\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(y_true)\n",
    "    top_labels = [label for label, _ in label_counts.most_common(20)]\n",
    "    \n",
    "    # Filter confusion matrix for top labels\n",
    "    label_indices = [unique_labels.index(label) for label in top_labels]\n",
    "    conf_matrix_subset = conf_matrix[np.ix_(label_indices, label_indices)]\n",
    "    \n",
    "    # Plot subset\n",
    "    sns.heatmap(conf_matrix_subset, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=top_labels,\n",
    "                yticklabels=top_labels,\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'Confusion Matrix (Top 20 Labels)\\nTotal Labels: {len(unique_labels)}', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "else:\n",
    "    # Plot full confusion matrix\n",
    "    sns.heatmap(conf_matrix, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=unique_labels,\n",
    "                yticklabels=unique_labels,\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title('Confusion Matrix - All Labels', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix visualization created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics with confusion matrix\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CALCULATING COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Extract labels from predictions and expected outputs\n",
    "# This assumes the output contains MITRE technique IDs (e.g., T1234)\n",
    "import re\n",
    "\n",
    "def extract_technique_id(text):\n",
    "    \"\"\"Extract MITRE technique ID from text (e.g., T1234, T1234.001)\"\"\"\n",
    "    match = re.search(r'T\\d{4}(?:\\.\\d{3})?', text.upper())\n",
    "    return match.group(0) if match else \"UNKNOWN\"\n",
    "\n",
    "# Extract all labels\n",
    "y_true = [extract_technique_id(r['expected']) for r in results]\n",
    "y_pred = [extract_technique_id(r['predicted']) for r in results]\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = sorted(list(set(y_true + y_pred)))\n",
    "print(f\"üìã Unique labels found: {len(unique_labels)}\")\n",
    "print(f\"   Labels: {', '.join(unique_labels[:10])}{'...' if len(unique_labels) > 10 else ''}\\n\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"üéØ OVERALL METRICS:\")\n",
    "print(f\"   Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\n   Precision (Macro):  {precision_macro:.4f}\")\n",
    "print(f\"   Precision (Weighted): {precision_weighted:.4f}\")\n",
    "print(f\"\\n   Recall (Macro):     {recall_macro:.4f}\")\n",
    "print(f\"   Recall (Weighted):  {recall_weighted:.4f}\")\n",
    "print(f\"\\n   F1-Score (Macro):   {f1_macro:.4f}\")\n",
    "print(f\"   F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Calculate word-level metrics (from previous evaluation)\n",
    "avg_partial_match = np.mean(partial_match_scores)\n",
    "avg_f1_word = np.mean(f1_scores)\n",
    "exact_match_accuracy = exact_matches / len(eval_samples)\n",
    "\n",
    "print(f\"\\nüìù WORD-LEVEL METRICS:\")\n",
    "print(f\"   Exact Match Accuracy: {exact_match_accuracy:.4f} ({exact_match_accuracy*100:.2f}%)\")\n",
    "print(f\"   Avg Partial Match:    {avg_partial_match:.4f}\")\n",
    "print(f\"   Avg F1 (Word-level):  {avg_f1_word:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics calculated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9103706,
     "sourceId": 14266243,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9170369,
     "sourceId": 14361280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
