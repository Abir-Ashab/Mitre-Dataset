{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:20:43.563000Z",
     "iopub.status.busy": "2025-12-31T05:20:43.562639Z",
     "iopub.status.idle": "2025-12-31T05:20:43.899729Z",
     "shell.execute_reply": "2025-12-31T05:20:43.898390Z",
     "shell.execute_reply.started": "2025-12-31T05:20:43.562970Z"
    },
    "id": "OxsWWU76ypYt",
    "outputId": "5c3e7a6c-a67c-4ef6-d54b-16fa58a57d1f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:20:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0             31W /   70W |    1617MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   53C    P0             30W /   70W |    1719MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:20:49.555921Z",
     "iopub.status.busy": "2025-12-31T05:20:49.555468Z",
     "iopub.status.idle": "2025-12-31T05:20:54.258653Z",
     "shell.execute_reply": "2025-12-31T05:20:54.258033Z",
     "shell.execute_reply.started": "2025-12-31T05:20:49.555885Z"
    },
    "id": "_Us5viIU19TI",
    "outputId": "cd14f111-e640-48c0-8323-f368cb27521b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Files in data folder:\n",
      "   val.jsonl: 48.9 MB\n",
      "   test.jsonl: 57.1 MB\n",
      "   train.jsonl: 395.8 MB\n",
      "âœ… train.jsonl: 31,409 examples\n",
      "âœ… val.jsonl: 3,717 examples\n",
      "âœ… test.jsonl: 4,352 examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Kaggle data path\n",
    "data_path = \"/kaggle/input/mitre-datset\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    print(\"ðŸ“‚ Files in data folder:\")\n",
    "    for f in files:\n",
    "        size = os.path.getsize(os.path.join(data_path, f)) / (1024**2)\n",
    "        print(f\"   {f}: {size:.1f} MB\")\n",
    "    \n",
    "    # Verify JSONL\n",
    "    import json\n",
    "    for filename in ['train.jsonl', 'val.jsonl', 'test.jsonl']:\n",
    "        filepath = os.path.join(data_path, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                count = sum(1 for _ in f)\n",
    "            print(f\"âœ… {filename}: {count:,} examples\")\n",
    "        else:\n",
    "            print(f\"âŒ {filename}: NOT FOUND\")\n",
    "else:\n",
    "    print(\"âŒ Dataset not found!\")\n",
    "    print(\"ðŸ“Œ Upload data: Click 'Add Data' > 'Upload' > 'New Dataset'\")\n",
    "    print(\"   Name it: mitre-dataset\")\n",
    "    print(\"   Upload: train.jsonl, val.jsonl, test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:22:55.084674Z",
     "iopub.status.busy": "2025-12-31T05:22:55.084106Z",
     "iopub.status.idle": "2025-12-31T05:22:59.329161Z",
     "shell.execute_reply": "2025-12-31T05:22:59.328444Z",
     "shell.execute_reply.started": "2025-12-31T05:22:55.084642Z"
    },
    "id": "nllTwEjk2W6v",
    "outputId": "84ad42ec-11b2-40cf-99d5-743628ae19d7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing dependencies...\n",
      "âœ… Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (no bitsandbytes for FP16 training)\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "!pip install -q transformers datasets accelerate peft\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:23:02.284790Z",
     "iopub.status.busy": "2025-12-31T05:23:02.284458Z",
     "iopub.status.idle": "2025-12-31T05:23:02.290636Z",
     "shell.execute_reply": "2025-12-31T05:23:02.289918Z",
     "shell.execute_reply.started": "2025-12-31T05:23:02.284757Z"
    },
    "id": "ft7IA_LO2jAh",
    "outputId": "ea780d55-a30e-48cd-f085-545965ecee00",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded (Kaggle)\n",
      "   Model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "   Epochs: 2\n",
      "   Estimated time: ~3-4 hours\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Configuration for Kaggle\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # 1.5B fits well in GPU\n",
    "\n",
    "# Kaggle paths (CHANGED from Colab)\n",
    "DATA_PATH = \"/kaggle/input/mitre-datset\"\n",
    "OUTPUT_DIR = \"/kaggle/working/checkpoints\"\n",
    "FINAL_MODEL_DIR = \"/kaggle/working/fine_tuned_model\"\n",
    "\n",
    "TRAIN_FILE = f\"{DATA_PATH}/train.jsonl\"\n",
    "VAL_FILE = f\"{DATA_PATH}/val.jsonl\"\n",
    "\n",
    "# Training settings\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "NUM_EPOCHS = 2           # 2 epochs for good fine-tuning\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "print(\"âœ… Configuration loaded (Kaggle)\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Estimated time: ~3-4 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "dea7e0c66a8e4807831ceb761abcd19c",
      "14af00e9fee84130a851af73dcb54954",
      "84957a4b45d64ed599b6fd6d6a26c38a",
      "b4303b3066cd42d3b456368d7bba01d3",
      "17f6ef3c8ed54c5babfa6ce09d241427",
      "5045fede23f048e787d057175febf279",
      "03ffadd6242f4a6aa54f6c273feb4982",
      "bcbdb357421f4fbf9295890dcfa9012a",
      "ec65741d34544d19a1b4aa81fc3bb132",
      "f59f122813254af3beb3e4c9c7884e12",
      "3544f8531fe94c4c8260a9639d26afcd"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:23:07.181982Z",
     "iopub.status.busy": "2025-12-31T05:23:07.181308Z",
     "iopub.status.idle": "2025-12-31T05:23:27.158463Z",
     "shell.execute_reply": "2025-12-31T05:23:27.157762Z",
     "shell.execute_reply.started": "2025-12-31T05:23:07.181952Z"
    },
    "id": "MzkwCjd0K3pY",
    "outputId": "22b661dd-8565-4e4b-ec9f-32f50e4fe2e4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b32abd9090d49078588c527ecdbe723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207d46c5f8bb438dab6d64e044c01e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63913e0f691a462884cbb8de6793cedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a13f222fa274445b6b6af3843d59c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dad718963be459aaae69aef1eb62b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959df6fb35cd4a6fbe33b5dae2440840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7244875a15840d7a1558cf89b20f3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded: Qwen/Qwen2.5-1.5B-Instruct\n",
      "ðŸ“Š GPU Memory: 2.88 GB / 15 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load model WITHOUT quantization (T4 has 15GB, should fit)\n",
    "print(\"\\nðŸ”„ Loading model...\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use FP16 instead of 4-bit\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"âœ… Model loaded: {MODEL_NAME}\")\n",
    "print(f\"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 15 GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:23:31.496188Z",
     "iopub.status.busy": "2025-12-31T05:23:31.495534Z",
     "iopub.status.idle": "2025-12-31T05:23:31.733364Z",
     "shell.execute_reply": "2025-12-31T05:23:31.732602Z",
     "shell.execute_reply.started": "2025-12-31T05:23:31.496158Z"
    },
    "id": "-4Nj92VwO1JZ",
    "outputId": "f1e1c788-3207-4ef9-8618-5d8b3f041448",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Configuring LoRA...\n",
      "trainable params: 4,358,144 || all params: 1,548,072,448 || trainable%: 0.2815\n",
      "âœ… LoRA configured\n",
      "ðŸ“Š GPU Memory: 2.89 GB / 15 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Configure LoRA (REQUIRED!)\n",
    "print(\"\\nðŸ”„ Configuring LoRA...\")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Enable gradient checkpointing and disable cache\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False  # CRITICAL: Required for gradient checkpointing\n",
    "\n",
    "# Prepare model for training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze base model\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32)  # Cast layer norms to fp32\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… LoRA configured\")\n",
    "print(f\"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 15 GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:23:35.723540Z",
     "iopub.status.busy": "2025-12-31T05:23:35.723241Z",
     "iopub.status.idle": "2025-12-31T05:23:39.756999Z",
     "shell.execute_reply": "2025-12-31T05:23:39.756341Z",
     "shell.execute_reply.started": "2025-12-31T05:23:35.723514Z"
    },
    "id": "p5VZkK9CPMY0",
    "outputId": "331531ce-0610-48f7-b013-0ae326a15b57",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cd70fea2254a5db1bf333593855d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497e309363084c4eb84c029fa0bcfff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded:\n",
      "   Training: 31409 examples\n",
      "   Validation: 3717 examples\n",
      "\n",
      "ðŸ“‹ Sample entry:\n",
      "{'instruction': 'Analyze this session log chunk and determine if it contains normal or suspicious activity. If suspicious, identify all MITRE ATT&CK techniques and explain why.', 'input': '{\"metadata\": {\"session_id\": \"20251025_163600\", \"chunk_index\": 220, \"start_time\": \"2025-10-25T10:42:06.341Z\", \"end_time\": \"2025-10-25T10:42:06.351Z\", \"number_of_events\": 20}, \"logs\": [{\"timestamp\": \"2025-10-25T10:42:06.341Z\", \"event_type\": \"network\", \"length\": 107, \"summary\": \"Ether / IPv6 / UDP / mDNS Qry b\\'cefalo(2)._dosvc._tcp.local.\\'\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"53\", \"nh\": \"17\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"UDP\": {\"sport\": \"5353\", \"dport\": \"5353\"}, \"DNS\": {\"id\": \"0\", \"qr\": \"0\", \"opcode\": \"0\", \"aa\": \"0\", \"tc\": \"0\", \"rd\": \"0\", \"ra\": \"0\", \"z\": \"0\", \"ad\": \"0\", \"cd\": \"0\", \"rcode\": \"0\", \"qdcount\": \"1\", \"ancount\": \"0\", \"nscount\": \"0\", \"arcount\": \"0\", \"qd\": \"[<DNSQR  qname=b\\'cefalo(2)._dosvc._tcp.local.\\' qtype=ALL unicastresponse=0 qclass=IN |>]\", \"an\": \"[]\", \"ns\": \"[]\", \"ar\": \"[]\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / IPv6 / IPv6ExtHdrFragment / UDP / mDNS Ans\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"0\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}, \"UDP\": {\"sport\": \"5353\", \"dport\": \"5353\"}, \"DNS\": {\"id\": \"0\", \"qr\": \"1\", \"opcode\": \"0\", \"aa\": \"1\", \"tc\": \"0\", \"rd\": \"0\", \"ra\": \"0\", \"z\": \"0\", \"ad\": \"0\", \"cd\": \"0\", \"rcode\": \"0\", \"qdcount\": \"0\", \"ancount\": \"86\", \"nscount\": \"0\", \"arcount\": \"173\", \"qd\": \"[]\", \"an\": \"[<DNSRRSRV  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=SRV cacheflush=0 rclass=IN ttl=120 priority=0 weight=0 port=7680 target=b\\'cefalo.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(1)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(4)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(20)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(3)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(18)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(5)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(12)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(13)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(14)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(15)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(16)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(17)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(19)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(21)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(40)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(41)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(42)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(43)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(44)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(45)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(46)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(47)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(48)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(49)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(50)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(51)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(52)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(53)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(54)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(55)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(56)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(57)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(7)._dosvc.\\' |>]\", \"ns\": \"[]\", \"ar\": \"[]\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"181\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"362\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"543\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"724\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"905\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1086\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1267\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1448\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1629\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1810\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.347Z\", \"event_type\": \"network\", \"length\": 963, \"summary\": \"Ether / fe80::79ec:5385:3926:7cbd > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"870203\", \"plen\": \"909\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::79ec:5385:3926:7cbd\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1991\", \"res2\": \"0\", \"m\": \"0\", \"id\": \"14024\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.348Z\", \"event_type\": \"network\", \"length\": 1063, \"summary\": \"Ether / 10.10.14.105 > 224.0.0.251 udp frag:2035 / Raw\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.14.105\", \"dst\": \"224.0.0.251\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.348Z\", \"event_type\": \"network\", \"length\": 1510, \"summary\": \"Ether / fe80::6581:6f19:c1db:50be > ff02::fb (44) / IPv6ExtHdrFragment / Raw\", \"layers\": {\"IPv6\": {\"version\": \"6\", \"tc\": \"0\", \"fl\": \"66792\", \"plen\": \"1456\", \"nh\": \"44\", \"hlim\": \"1\", \"src\": \"fe80::6581:6f19:c1db:50be\", \"dst\": \"ff02::fb\"}, \"IPv6ExtHdrFragment\": {\"nh\": \"17\", \"res1\": \"0\", \"offset\": \"1086\", \"res2\": \"0\", \"m\": \"1\", \"id\": \"6750\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.351Z\", \"event_type\": \"network\", \"length\": 1514, \"summary\": \"Ether / IP / UDP / mDNS Ans\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.13.2\", \"dst\": \"224.0.0.251\"}, \"UDP\": {\"sport\": \"5353\", \"dport\": \"5353\"}, \"DNS\": {\"id\": \"0\", \"qr\": \"1\", \"opcode\": \"0\", \"aa\": \"1\", \"tc\": \"0\", \"rd\": \"0\", \"ra\": \"0\", \"z\": \"0\", \"ad\": \"0\", \"cd\": \"0\", \"rcode\": \"0\", \"qdcount\": \"0\", \"ancount\": \"86\", \"nscount\": \"0\", \"arcount\": \"173\", \"qd\": \"[]\", \"an\": \"[<DNSRRSRV  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=SRV cacheflush=0 rclass=IN ttl=120 priority=0 weight=0 port=7680 target=b\\'cefalo.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(1)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(4)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(20)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(3)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(18)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(5)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(12)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(13)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(14)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(15)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(16)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(17)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(19)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(21)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(40)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(41)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(42)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(43)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(44)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(45)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(46)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(47)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(48)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(49)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(50)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(51)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(52)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(53)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(54)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(55)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(56)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(57)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(7)._dosvc._tcp.local.\\' |>, <DNSRR  rrname=b\\'cefalo(2)._dosvc._tcp.local.\\' type=PTR cacheflush=0 rclass=IN ttl=4500 rdata=b\\'cefalo(.\\' |>]\", \"ns\": \"[]\", \"ar\": \"[]\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.351Z\", \"event_type\": \"network\", \"length\": 1514, \"summary\": \"Ether / 10.10.13.2 > 224.0.0.251 udp frag:185 / Raw\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.13.2\", \"dst\": \"224.0.0.251\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.351Z\", \"event_type\": \"network\", \"length\": 1514, \"summary\": \"Ether / 10.10.13.2 > 224.0.0.251 udp frag:370 / Raw\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.13.2\", \"dst\": \"224.0.0.251\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.351Z\", \"event_type\": \"network\", \"length\": 1514, \"summary\": \"Ether / 10.10.13.2 > 224.0.0.251 udp frag:555 / Raw\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.13.2\", \"dst\": \"224.0.0.251\"}}}, {\"timestamp\": \"2025-10-25T10:42:06.351Z\", \"event_type\": \"network\", \"length\": 1514, \"summary\": \"Ether / 10.10.13.2 > 224.0.0.251 udp frag:740 / Raw\", \"layers\": {\"IP\": {\"ttl\": \"1\", \"proto\": \"17\", \"src\": \"10.10.13.2\", \"dst\": \"224.0.0.251\"}}}]}', 'output': 'Status: Normal\\nReason: Standard system activity with no suspicious indicators across all events'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load dataset\n",
    "print(\"\\nðŸ”„ Loading dataset...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files={\n",
    "        'train': TRAIN_FILE,\n",
    "        'validation': VAL_FILE\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset loaded:\")\n",
    "print(f\"   Training: {len(dataset['train'])} examples\")\n",
    "print(f\"   Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"\\nðŸ“‹ Sample entry:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "80df8959943346319aa857a012675ec3",
      "fcd1bd417b9a4087b916d7aef75b51be",
      "26db7ffd01514fd69ccc6e1205083211",
      "9deb1dda55db4ae9a757a98724fc8634",
      "93eeb1b6efe2469f8fae0842fe182182",
      "a564b41fcdea4797a01a4fd01e2a7d8a",
      "3ab10afd37d6443c8a2d4ef7f35c5461",
      "b7539d2b59be4d3d9124d33732116090",
      "fba5a73dd7684c80a1fe395a6387cace",
      "9d4d2aa36da2402eae751a459dcc8745",
      "5a27bfcc07be4fe29efa405da57a3b67",
      "3c5983a49b9f46299f8e507fcefdd39c",
      "b1352830aaa443e69f0a0205aa81e0e9",
      "ea377145d602424492736c9b3d455d12",
      "51ae826fe33643edb97775174439dfdb",
      "bec83df5b3ad42faac9123b997de2169",
      "d2cf22c7bc634e48a3e12c398c8c04b9",
      "877c9f821c9641ca9815761c9914fa76",
      "9cbb8c048f59433abe10c51618bfdc9b",
      "d7ce60214b664461b6b4c64ceb468eae",
      "72df0825a21c4b98afe1c0fe34e5b9c4",
      "9a0141597ff844c5bb2ca50bae7a46ea"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:23:45.745943Z",
     "iopub.status.busy": "2025-12-31T05:23:45.744945Z",
     "iopub.status.idle": "2025-12-31T05:26:38.873119Z",
     "shell.execute_reply": "2025-12-31T05:26:38.872354Z",
     "shell.execute_reply.started": "2025-12-31T05:23:45.745910Z"
    },
    "id": "GhRQVH-dPQ6u",
    "outputId": "bb1ddc12-7292-4429-fc52-b6ca5a46f745",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Formatting and tokenizing dataset...\n",
      "Dataset columns: ['instruction', 'input', 'output']\n",
      "\n",
      "ðŸ”„ Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a62ad287304790868e89734e75398f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a7d098c50641409929ea4f1750c148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset tokenized\n",
      "   Training: 31409 examples\n",
      "   Validation: 3717 examples\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Format and tokenize dataset\n",
    "print(\"\\nðŸ”„ Formatting and tokenizing dataset...\")\n",
    "\n",
    "# Dataset has columns: instruction, input, output\n",
    "print(f\"Dataset columns: {dataset['train'].column_names}\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    # Combine instruction + input + output into training format\n",
    "    return f\"\"\"{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Format each example\n",
    "    texts = [\n",
    "        format_prompt({\n",
    "            'instruction': inst,\n",
    "            'input': inp,\n",
    "            'output': out\n",
    "        })\n",
    "        for inst, inp, out in zip(\n",
    "            examples['instruction'],\n",
    "            examples['input'],\n",
    "            examples['output']\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Add labels (same as input_ids for causal LM)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "print(\"\\nðŸ”„ Tokenizing...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset tokenized\")\n",
    "print(f\"   Training: {len(tokenized_dataset['train'])} examples\")\n",
    "print(f\"   Validation: {len(tokenized_dataset['validation'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:27:48.986085Z",
     "iopub.status.busy": "2025-12-31T05:27:48.985457Z",
     "iopub.status.idle": "2025-12-31T05:27:49.066363Z",
     "shell.execute_reply": "2025-12-31T05:27:49.065760Z",
     "shell.execute_reply.started": "2025-12-31T05:27:48.986054Z"
    },
    "id": "VWB99fCbRLRW",
    "outputId": "58ee35ea-2d37-4422-fc7a-c1619d526445",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Setting up training arguments...\n",
      "âœ… Training arguments configured\n",
      "   Batch size: 1 (reduced for memory)\n",
      "   Effective batch size: 8 (still 8 total)\n",
      "   Total steps: ~7852\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Set up training arguments (REDUCED for memory)\n",
    "print(\"\\nðŸ”„ Setting up training arguments...\")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=1,              # Reduced from 2 to 1\n",
    "    per_device_eval_batch_size=1,               # Reduced from 2 to 1\n",
    "    gradient_accumulation_steps=8,              # Increased from 4 to 8\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    warmup_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured\")\n",
    "print(f\"   Batch size: 1 (reduced for memory)\")\n",
    "print(f\"   Effective batch size: {1 * 8} (still 8 total)\")\n",
    "print(f\"   Total steps: ~{len(tokenized_dataset['train']) // (1 * 8) * NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:27:52.176246Z",
     "iopub.status.busy": "2025-12-31T05:27:52.175948Z",
     "iopub.status.idle": "2025-12-31T05:27:53.462676Z",
     "shell.execute_reply": "2025-12-31T05:27:53.462101Z",
     "shell.execute_reply.started": "2025-12-31T05:27:52.176220Z"
    },
    "id": "s5w9caxMRNtG",
    "outputId": "e80d4b11-ca25-436c-d5cf-527030e7f51a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Creating trainer...\n",
      "âœ… Trainer created\n",
      "ðŸ“Š GPU Memory: 2.89 GB / 15 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Create Trainer\n",
    "print(\"\\nðŸ”„ Creating trainer...\")\n",
    "\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer created\")\n",
    "print(f\"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 15 GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "execution": {
     "iopub.execute_input": "2025-12-31T05:30:44.788870Z",
     "iopub.status.busy": "2025-12-31T05:30:44.788179Z",
     "iopub.status.idle": "2025-12-31T14:58:08.501680Z",
     "shell.execute_reply": "2025-12-31T14:58:08.501096Z",
     "shell.execute_reply.started": "2025-12-31T05:30:44.788832Z"
    },
    "id": "oQTr1JXxRPpm",
    "outputId": "5e255933-c472-42a7-aa3b-33a9d7e2a775",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting training...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7854' max='7854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7854/7854 9:27:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.253941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.232756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.224292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.226155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.224059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.215197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.216673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.220824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.219608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.220055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.221761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.221792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.220499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.222165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.224638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training completed in 9.46 hours\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Start training\n",
    "print(\"\\nðŸš€ Starting training...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Training completed in {elapsed_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:58:08.503184Z",
     "iopub.status.busy": "2025-12-31T14:58:08.502941Z",
     "iopub.status.idle": "2025-12-31T14:58:08.869159Z",
     "shell.execute_reply": "2025-12-31T14:58:08.868413Z",
     "shell.execute_reply.started": "2025-12-31T14:58:08.503163Z"
    },
    "id": "2dxkHqoHRSJX",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saving model...\n",
      "âœ… Model saved to: /kaggle/working/fine_tuned_model\n",
      "\n",
      "ðŸ“¥ Download model:\n",
      "   See next cell to create download link\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save the fine-tuned model\n",
    "print(\"\\nðŸ’¾ Saving model...\")\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(f\"âœ… Model saved to: {FINAL_MODEL_DIR}\")\n",
    "print(\"\\nðŸ“¥ Download model:\")\n",
    "print(\"   See next cell to create download link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:58:08.870579Z",
     "iopub.status.busy": "2025-12-31T14:58:08.870208Z",
     "iopub.status.idle": "2025-12-31T14:58:10.813715Z",
     "shell.execute_reply": "2025-12-31T14:58:10.813015Z",
     "shell.execute_reply.started": "2025-12-31T14:58:08.870545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Creating downloadable zip...\n",
      "  adding: kaggle/working/fine_tuned_model/ (stored 0%)\n",
      "  adding: kaggle/working/fine_tuned_model/tokenizer.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 81%)\n",
      "  adding: kaggle/working/fine_tuned_model/added_tokens.json (deflated 67%)\n",
      "  adding: kaggle/working/fine_tuned_model/vocab.json (deflated 61%)\n",
      "  adding: kaggle/working/fine_tuned_model/special_tokens_map.json (deflated 63%)\n",
      "  adding: kaggle/working/fine_tuned_model/tokenizer_config.json (deflated 89%)\n",
      "  adding: kaggle/working/fine_tuned_model/README.md (deflated 65%)\n",
      "  adding: kaggle/working/fine_tuned_model/chat_template.jinja (deflated 71%)\n",
      "  adding: kaggle/working/fine_tuned_model/adapter_config.json (deflated 56%)\n",
      "  adding: kaggle/working/fine_tuned_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/fine_tuned_model/merges.txt (deflated 57%)\n",
      "âœ… Model zipped: fine_tuned_model.zip\n",
      "   Size: 19.2 MB\n",
      "\n",
      "ðŸ“¥ Download: Check the 'Output' section on the right\n",
      "   Click on fine_tuned_model.zip to download\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Download model from Kaggle\n",
    "print(\"ðŸ“¦ Creating downloadable zip...\")\n",
    "\n",
    "# Zip the model\n",
    "!zip -r fine_tuned_model.zip {FINAL_MODEL_DIR}\n",
    "\n",
    "print(f\"âœ… Model zipped: fine_tuned_model.zip\")\n",
    "print(f\"   Size: {os.path.getsize('fine_tuned_model.zip') / (1024**2):.1f} MB\")\n",
    "print(\"\\nðŸ“¥ Download: Check the 'Output' section on the right\")\n",
    "print(\"   Click on fine_tuned_model.zip to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Now let's evaluate the fine-tuned model on the test dataset to measure its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Cell 14: Load the fine-tuned model for evaluation\n",
    "print(\"ðŸ”„ Loading fine-tuned model for evaluation...\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Use the model path from training (Kaggle)\n",
    "# If FINAL_MODEL_DIR doesn't exist, use default Kaggle path\n",
    "try:\n",
    "    EVAL_MODEL_PATH = FINAL_MODEL_DIR\n",
    "except NameError:\n",
    "    EVAL_MODEL_PATH = \"/kaggle/working/fine_tuned_model\"\n",
    "\n",
    "print(f\"Loading model from: {EVAL_MODEL_PATH}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    EVAL_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(EVAL_MODEL_PATH, trust_remote_code=True)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "eval_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"âœ… Model loaded from: {EVAL_MODEL_PATH}\")\n",
    "print(f\"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Load test dataset\n",
    "print(\"ðŸ”„ Loading test dataset...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Use DATA_PATH from training configuration (Kaggle)\n",
    "try:\n",
    "    TEST_FILE = f\"{DATA_PATH}/test.jsonl\"\n",
    "except NameError:\n",
    "    # Default Kaggle path if DATA_PATH not defined\n",
    "    TEST_FILE = \"/kaggle/input/mitre-datset/test.jsonl\"\n",
    "\n",
    "print(f\"Loading test data from: {TEST_FILE}\")\n",
    "\n",
    "test_dataset = load_dataset('json', data_files={'test': TEST_FILE})['test']\n",
    "\n",
    "print(f\"âœ… Test dataset loaded: {len(test_dataset)} examples\")\n",
    "print(f\"\\nðŸ“‹ Sample test entry:\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Define evaluation function\n",
    "print(\"ðŸ”„ Defining evaluation functions...\")\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, input_text, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given instruction and input.\"\"\"\n",
    "    prompt = f\"\"\"{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (remove the prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "def calculate_exact_match(pred, target):\n",
    "    \"\"\"Calculate exact match accuracy.\"\"\"\n",
    "    return 1.0 if pred.strip().lower() == target.strip().lower() else 0.0\n",
    "\n",
    "def calculate_partial_match(pred, target):\n",
    "    \"\"\"Calculate partial match (if prediction contains key parts of target).\"\"\"\n",
    "    pred_lower = pred.strip().lower()\n",
    "    target_lower = target.strip().lower()\n",
    "    \n",
    "    # Check if major keywords from target appear in prediction\n",
    "    target_words = set(target_lower.split())\n",
    "    pred_words = set(pred_lower.split())\n",
    "    \n",
    "    if len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(target_words.intersection(pred_words))\n",
    "    return overlap / len(target_words)\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Run evaluation on test set\n",
    "print(\"ðŸ”„ Running evaluation on test set...\")\n",
    "print(f\"   This may take a while for {len(test_dataset)} examples...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Limit evaluation to first N examples for speed (optional)\n",
    "EVAL_LIMIT = 100  # Change to len(test_dataset) for full evaluation\n",
    "eval_samples = test_dataset.select(range(min(EVAL_LIMIT, len(test_dataset))))\n",
    "\n",
    "results = []\n",
    "exact_matches = 0\n",
    "partial_match_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_samples, desc=\"Evaluating\")):\n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        eval_model,\n",
    "        eval_tokenizer,\n",
    "        example['instruction'],\n",
    "        example['input']\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = calculate_exact_match(prediction, example['output'])\n",
    "    partial_match = calculate_partial_match(prediction, example['output'])\n",
    "    \n",
    "    exact_matches += exact_match\n",
    "    partial_match_scores.append(partial_match)\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'instruction': example['instruction'],\n",
    "        'input': example['input'],\n",
    "        'expected': example['output'],\n",
    "        'predicted': prediction,\n",
    "        'exact_match': exact_match,\n",
    "        'partial_match': partial_match\n",
    "    })\n",
    "    \n",
    "    # Show first few examples\n",
    "    if i < 3:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Input: {example['input'][:100]}...\")\n",
    "        print(f\"Expected: {example['output'][:100]}...\")\n",
    "        print(f\"Predicted: {prediction[:100]}...\")\n",
    "        print(f\"Exact Match: {exact_match}, Partial Match: {partial_match:.2f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Evaluation completed in {elapsed/60:.2f} minutes\")\n",
    "print(f\"\\nðŸ“Š ACCURACY METRICS (on {len(eval_samples)} samples):\")\n",
    "print(f\"   Exact Match Accuracy: {exact_matches / len(eval_samples) * 100:.2f}%\")\n",
    "print(f\"   Average Partial Match: {sum(partial_match_scores) / len(partial_match_scores) * 100:.2f}%\")\n",
    "print(f\"   Total Samples: {len(eval_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Detailed analysis of results\n",
    "print(\"ðŸ“Š Detailed Analysis:\\n\")\n",
    "\n",
    "# Show distribution of partial match scores\n",
    "import numpy as np\n",
    "\n",
    "partial_scores_array = np.array(partial_match_scores)\n",
    "\n",
    "print(f\"Partial Match Score Distribution:\")\n",
    "print(f\"   Min: {partial_scores_array.min():.2f}\")\n",
    "print(f\"   Max: {partial_scores_array.max():.2f}\")\n",
    "print(f\"   Mean: {partial_scores_array.mean():.2f}\")\n",
    "print(f\"   Median: {np.median(partial_scores_array):.2f}\")\n",
    "print(f\"   Std Dev: {partial_scores_array.std():.2f}\")\n",
    "\n",
    "# Count by score ranges\n",
    "ranges = [\n",
    "    (0.0, 0.2, \"Very Poor (0-20%)\"),\n",
    "    (0.2, 0.4, \"Poor (20-40%)\"),\n",
    "    (0.4, 0.6, \"Fair (40-60%)\"),\n",
    "    (0.6, 0.8, \"Good (60-80%)\"),\n",
    "    (0.8, 1.0, \"Excellent (80-100%)\")\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Score Distribution:\")\n",
    "for low, high, label in ranges:\n",
    "    count = sum(1 for score in partial_scores_array if low <= score < high)\n",
    "    percentage = count / len(partial_scores_array) * 100\n",
    "    print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show examples of best and worst predictions\n",
    "print(f\"\\nâœ… BEST Predictions (highest partial match):\")\n",
    "sorted_results = sorted(results, key=lambda x: x['partial_match'], reverse=True)\n",
    "for i, result in enumerate(sorted_results[:3]):\n",
    "    print(f\"\\n{i+1}. Score: {result['partial_match']:.2f}\")\n",
    "    print(f\"   Input: {result['input'][:80]}...\")\n",
    "    print(f\"   Expected: {result['expected'][:80]}...\")\n",
    "    print(f\"   Predicted: {result['predicted'][:80]}...\")\n",
    "\n",
    "print(f\"\\nâŒ WORST Predictions (lowest partial match):\")\n",
    "for i, result in enumerate(sorted_results[-3:]):\n",
    "    print(f\"\\n{i+1}. Score: {result['partial_match']:.2f}\")\n",
    "    print(f\"   Input: {result['input'][:80]}...\")\n",
    "    print(f\"   Expected: {result['expected'][:80]}...\")\n",
    "    print(f\"   Predicted: {result['predicted'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Save evaluation results (optional)\n",
    "print(\"ðŸ’¾ Saving evaluation results...\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Save as JSON\n",
    "results_file = \"evaluation_results.json\"\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save as CSV for easier viewing\n",
    "df = pd.DataFrame(results)\n",
    "csv_file = \"evaluation_results.csv\"\n",
    "df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… Results saved:\")\n",
    "print(f\"   JSON: {results_file}\")\n",
    "print(f\"   CSV: {csv_file}\")\n",
    "print(f\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(df[['exact_match', 'partial_match']].describe())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9103706,
     "sourceId": 14266243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03ffadd6242f4a6aa54f6c273feb4982": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14af00e9fee84130a851af73dcb54954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5045fede23f048e787d057175febf279",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_03ffadd6242f4a6aa54f6c273feb4982",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "17f6ef3c8ed54c5babfa6ce09d241427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26db7ffd01514fd69ccc6e1205083211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7539d2b59be4d3d9124d33732116090",
      "max": 31409,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fba5a73dd7684c80a1fe395a6387cace",
      "value": 31409
     }
    },
    "3544f8531fe94c4c8260a9639d26afcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ab10afd37d6443c8a2d4ef7f35c5461": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c5983a49b9f46299f8e507fcefdd39c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1352830aaa443e69f0a0205aa81e0e9",
       "IPY_MODEL_ea377145d602424492736c9b3d455d12",
       "IPY_MODEL_51ae826fe33643edb97775174439dfdb"
      ],
      "layout": "IPY_MODEL_bec83df5b3ad42faac9123b997de2169"
     }
    },
    "5045fede23f048e787d057175febf279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51ae826fe33643edb97775174439dfdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72df0825a21c4b98afe1c0fe34e5b9c4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9a0141597ff844c5bb2ca50bae7a46ea",
      "value": "â€‡3717/3717â€‡[00:44&lt;00:00,â€‡86.95â€‡examples/s]"
     }
    },
    "5a27bfcc07be4fe29efa405da57a3b67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72df0825a21c4b98afe1c0fe34e5b9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80df8959943346319aa857a012675ec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fcd1bd417b9a4087b916d7aef75b51be",
       "IPY_MODEL_26db7ffd01514fd69ccc6e1205083211",
       "IPY_MODEL_9deb1dda55db4ae9a757a98724fc8634"
      ],
      "layout": "IPY_MODEL_93eeb1b6efe2469f8fae0842fe182182"
     }
    },
    "84957a4b45d64ed599b6fd6d6a26c38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcbdb357421f4fbf9295890dcfa9012a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec65741d34544d19a1b4aa81fc3bb132",
      "value": 2
     }
    },
    "877c9f821c9641ca9815761c9914fa76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93eeb1b6efe2469f8fae0842fe182182": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a0141597ff844c5bb2ca50bae7a46ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cbb8c048f59433abe10c51618bfdc9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d4d2aa36da2402eae751a459dcc8745": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9deb1dda55db4ae9a757a98724fc8634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d4d2aa36da2402eae751a459dcc8745",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5a27bfcc07be4fe29efa405da57a3b67",
      "value": "â€‡31409/31409â€‡[06:09&lt;00:00,â€‡93.26â€‡examples/s]"
     }
    },
    "a564b41fcdea4797a01a4fd01e2a7d8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1352830aaa443e69f0a0205aa81e0e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2cf22c7bc634e48a3e12c398c8c04b9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_877c9f821c9641ca9815761c9914fa76",
      "value": "Map:â€‡100%"
     }
    },
    "b4303b3066cd42d3b456368d7bba01d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f59f122813254af3beb3e4c9c7884e12",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3544f8531fe94c4c8260a9639d26afcd",
      "value": "â€‡2/2â€‡[00:30&lt;00:00,â€‡14.75s/it]"
     }
    },
    "b7539d2b59be4d3d9124d33732116090": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcbdb357421f4fbf9295890dcfa9012a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bec83df5b3ad42faac9123b997de2169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2cf22c7bc634e48a3e12c398c8c04b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7ce60214b664461b6b4c64ceb468eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dea7e0c66a8e4807831ceb761abcd19c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14af00e9fee84130a851af73dcb54954",
       "IPY_MODEL_84957a4b45d64ed599b6fd6d6a26c38a",
       "IPY_MODEL_b4303b3066cd42d3b456368d7bba01d3"
      ],
      "layout": "IPY_MODEL_17f6ef3c8ed54c5babfa6ce09d241427"
     }
    },
    "ea377145d602424492736c9b3d455d12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cbb8c048f59433abe10c51618bfdc9b",
      "max": 3717,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7ce60214b664461b6b4c64ceb468eae",
      "value": 3717
     }
    },
    "ec65741d34544d19a1b4aa81fc3bb132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f59f122813254af3beb3e4c9c7884e12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fba5a73dd7684c80a1fe395a6387cace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fcd1bd417b9a4087b916d7aef75b51be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a564b41fcdea4797a01a4fd01e2a7d8a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3ab10afd37d6443c8a2d4ef7f35c5461",
      "value": "Map:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
