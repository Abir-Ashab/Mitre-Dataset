{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87548565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1aa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "!pip install -q transformers datasets accelerate peft tqdm pandas numpy\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Kaggle paths - UPDATE THESE based on your uploaded dataset names\n",
    "MODEL_PATH = \"/kaggle/input/mitre-fine-tuned-model\"  # Your uploaded model dataset\n",
    "DATA_PATH = \"/kaggle/input/mitre-datset\"  # Your test data dataset\n",
    "TEST_FILE = f\"{DATA_PATH}/test.jsonl\"\n",
    "\n",
    "# Evaluation settings\n",
    "EVAL_LIMIT = None  # Set to None for full evaluation, or a number like 100 for quick test\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Test data: {TEST_FILE}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"âœ… Model found: {len(os.listdir(MODEL_PATH))} files\")\n",
    "else:\n",
    "    print(f\"âŒ Model not found at {MODEL_PATH}\")\n",
    "    print(\"   Please upload your fine_tuned_model as a Kaggle dataset\")\n",
    "\n",
    "if os.path.exists(TEST_FILE):\n",
    "    print(f\"âœ… Test file found\")\n",
    "else:\n",
    "    print(f\"âŒ Test file not found at {TEST_FILE}\")\n",
    "    print(\"   Please add your test dataset to Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b724af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "print(\"ðŸ”„ Loading fine-tuned model...\\n\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"âœ… Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"ðŸ“Š Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"ðŸ”„ Loading test dataset...\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset('json', data_files={'test': TEST_FILE})['test']\n",
    "\n",
    "print(f\"âœ… Test dataset loaded: {len(test_dataset):,} examples\")\n",
    "print(f\"\\nðŸ“‹ Dataset columns: {test_dataset.column_names}\")\n",
    "print(f\"\\nðŸ“‹ Sample test entry:\")\n",
    "print(f\"   Instruction: {test_dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"   Input: {test_dataset[0]['input'][:100]}...\")\n",
    "print(f\"   Output: {test_dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ed487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation functions\n",
    "print(\"ðŸ”„ Defining evaluation functions...\\n\")\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, input_text, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given instruction and input.\"\"\"\n",
    "    prompt = f\"\"\"{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (remove the prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "def calculate_exact_match(pred, target):\n",
    "    \"\"\"Calculate exact match accuracy.\"\"\"\n",
    "    return 1.0 if pred.strip().lower() == target.strip().lower() else 0.0\n",
    "\n",
    "def calculate_partial_match(pred, target):\n",
    "    \"\"\"Calculate partial match (keyword overlap).\"\"\"\n",
    "    pred_lower = pred.strip().lower()\n",
    "    target_lower = target.strip().lower()\n",
    "    \n",
    "    # Check if major keywords from target appear in prediction\n",
    "    target_words = set(target_lower.split())\n",
    "    pred_words = set(pred_lower.split())\n",
    "    \n",
    "    if len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(target_words.intersection(pred_words))\n",
    "    return overlap / len(target_words)\n",
    "\n",
    "def calculate_f1_score(pred, target):\n",
    "    \"\"\"Calculate F1 score based on word overlap.\"\"\"\n",
    "    pred_words = set(pred.strip().lower().split())\n",
    "    target_words = set(target.strip().lower().split())\n",
    "    \n",
    "    if len(pred_words) == 0 or len(target_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(pred_words.intersection(target_words))\n",
    "    \n",
    "    precision = overlap / len(pred_words) if len(pred_words) > 0 else 0.0\n",
    "    recall = overlap / len(target_words) if len(target_words) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"ðŸš€ Running evaluation on test set...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Determine sample size\n",
    "if EVAL_LIMIT is None:\n",
    "    eval_samples = test_dataset\n",
    "    print(f\"Evaluating on FULL test set: {len(eval_samples):,} examples\")\n",
    "else:\n",
    "    eval_samples = test_dataset.select(range(min(EVAL_LIMIT, len(test_dataset))))\n",
    "    print(f\"Evaluating on LIMITED test set: {len(eval_samples):,} examples (out of {len(test_dataset):,})\")\n",
    "\n",
    "print(f\"This may take a while...\\n\")\n",
    "\n",
    "results = []\n",
    "exact_matches = 0\n",
    "partial_match_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_samples, desc=\"Evaluating\")):\n",
    "    # Generate prediction\n",
    "    prediction = generate_response(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        example['instruction'],\n",
    "        example['input'],\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = calculate_exact_match(prediction, example['output'])\n",
    "    partial_match = calculate_partial_match(prediction, example['output'])\n",
    "    f1 = calculate_f1_score(prediction, example['output'])\n",
    "    \n",
    "    exact_matches += exact_match\n",
    "    partial_match_scores.append(partial_match)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'instruction': example['instruction'],\n",
    "        'input': example['input'],\n",
    "        'expected': example['output'],\n",
    "        'predicted': prediction,\n",
    "        'exact_match': exact_match,\n",
    "        'partial_match': partial_match,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    # Show first few examples\n",
    "    if i < 5:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Instruction: {example['instruction'][:80]}...\")\n",
    "        print(f\"Input: {example['input'][:100]}...\")\n",
    "        print(f\"Expected: {example['output'][:100]}...\")\n",
    "        print(f\"Predicted: {prediction[:100]}...\")\n",
    "        print(f\"Metrics: Exact={exact_match}, Partial={partial_match:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Evaluation completed in {elapsed/60:.2f} minutes ({elapsed/len(eval_samples):.2f} sec/example)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c66bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display main accuracy metrics\n",
    "import numpy as np\n",
    "\n",
    "partial_scores_array = np.array(partial_match_scores)\n",
    "f1_scores_array = np.array(f1_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest Set Size: {len(eval_samples):,} examples\")\n",
    "print(f\"\\nðŸŽ¯ PRIMARY METRICS:\")\n",
    "print(f\"   Exact Match Accuracy: {exact_matches / len(eval_samples) * 100:.2f}%\")\n",
    "print(f\"   Partial Match (Recall): {partial_scores_array.mean() * 100:.2f}%\")\n",
    "print(f\"   F1 Score: {f1_scores_array.mean() * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DETAILED STATISTICS:\")\n",
    "print(f\"\\nPartial Match Distribution:\")\n",
    "print(f\"   Mean: {partial_scores_array.mean():.3f}\")\n",
    "print(f\"   Median: {np.median(partial_scores_array):.3f}\")\n",
    "print(f\"   Std Dev: {partial_scores_array.std():.3f}\")\n",
    "print(f\"   Min: {partial_scores_array.min():.3f}\")\n",
    "print(f\"   Max: {partial_scores_array.max():.3f}\")\n",
    "\n",
    "print(f\"\\nF1 Score Distribution:\")\n",
    "print(f\"   Mean: {f1_scores_array.mean():.3f}\")\n",
    "print(f\"   Median: {np.median(f1_scores_array):.3f}\")\n",
    "print(f\"   Std Dev: {f1_scores_array.std():.3f}\")\n",
    "print(f\"   Min: {f1_scores_array.min():.3f}\")\n",
    "print(f\"   Max: {f1_scores_array.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution analysis\n",
    "print(\"\\nðŸ“Š SCORE DISTRIBUTION ANALYSIS\\n\")\n",
    "\n",
    "ranges = [\n",
    "    (0.0, 0.2, \"Very Poor (0-20%)\"),\n",
    "    (0.2, 0.4, \"Poor (20-40%)\"),\n",
    "    (0.4, 0.6, \"Fair (40-60%)\"),\n",
    "    (0.6, 0.8, \"Good (60-80%)\"),\n",
    "    (0.8, 1.0, \"Excellent (80-100%)\")\n",
    "]\n",
    "\n",
    "print(\"Partial Match Score Distribution:\")\n",
    "for low, high, label in ranges:\n",
    "    count = sum(1 for score in partial_scores_array if low <= score < high)\n",
    "    percentage = count / len(partial_scores_array) * 100\n",
    "    bar = \"â–ˆ\" * int(percentage / 2)\n",
    "    print(f\"   {label:25} {count:4d} ({percentage:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\nF1 Score Distribution:\")\n",
    "for low, high, label in ranges:\n",
    "    count = sum(1 for score in f1_scores_array if low <= score < high)\n",
    "    percentage = count / len(f1_scores_array) * 100\n",
    "    bar = \"â–ˆ\" * int(percentage / 2)\n",
    "    print(f\"   {label:25} {count:4d} ({percentage:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best predictions\n",
    "print(\"\\nâœ… TOP 5 BEST PREDICTIONS (highest F1 scores):\\n\")\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x['f1_score'], reverse=True)\n",
    "\n",
    "for i, result in enumerate(sorted_results[:5]):\n",
    "    print(f\"{i+1}. F1={result['f1_score']:.3f}, Partial={result['partial_match']:.3f}, Exact={result['exact_match']}\")\n",
    "    print(f\"   Input: {result['input'][:120]}...\")\n",
    "    print(f\"   Expected: {result['expected'][:120]}...\")\n",
    "    print(f\"   Predicted: {result['predicted'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d779e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show worst predictions\n",
    "print(\"\\nâŒ TOP 5 WORST PREDICTIONS (lowest F1 scores):\\n\")\n",
    "\n",
    "for i, result in enumerate(sorted_results[-5:]):\n",
    "    print(f\"{i+1}. F1={result['f1_score']:.3f}, Partial={result['partial_match']:.3f}, Exact={result['exact_match']}\")\n",
    "    print(f\"   Input: {result['input'][:120]}...\")\n",
    "    print(f\"   Expected: {result['expected'][:120]}...\")\n",
    "    print(f\"   Predicted: {result['predicted'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccf5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "print(\"ðŸ’¾ Saving evaluation results...\\n\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Save as JSON\n",
    "results_file = \"evaluation_results.json\"\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save as CSV\n",
    "df = pd.DataFrame(results)\n",
    "csv_file = \"evaluation_results.csv\"\n",
    "df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    \"test_set_size\": len(eval_samples),\n",
    "    \"exact_match_accuracy\": float(exact_matches / len(eval_samples)),\n",
    "    \"partial_match_mean\": float(partial_scores_array.mean()),\n",
    "    \"partial_match_std\": float(partial_scores_array.std()),\n",
    "    \"f1_score_mean\": float(f1_scores_array.mean()),\n",
    "    \"f1_score_std\": float(f1_scores_array.std()),\n",
    "    \"evaluation_time_minutes\": elapsed / 60\n",
    "}\n",
    "\n",
    "summary_file = \"evaluation_summary.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results saved:\")\n",
    "print(f\"   {results_file} ({os.path.getsize(results_file) / 1024:.1f} KB)\")\n",
    "print(f\"   {csv_file} ({os.path.getsize(csv_file) / 1024:.1f} KB)\")\n",
    "print(f\"   {summary_file}\")\n",
    "print(f\"\\nðŸ“¥ Download these files from the 'Output' section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization (optional - requires matplotlib)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram of partial match scores\n",
    "    axes[0].hist(partial_scores_array, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(partial_scores_array.mean(), color='red', linestyle='--', label=f'Mean: {partial_scores_array.mean():.2f}')\n",
    "    axes[0].set_xlabel('Partial Match Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Partial Match Scores')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Histogram of F1 scores\n",
    "    axes[1].hist(f1_scores_array, bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[1].axvline(f1_scores_array.mean(), color='red', linestyle='--', label=f'Mean: {f1_scores_array.mean():.2f}')\n",
    "    axes[1].set_xlabel('F1 Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of F1 Scores')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"âœ… Visualization saved: evaluation_distributions.png\")\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸ Matplotlib not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35959953",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The evaluation is complete! Key files generated:\n",
    "- `evaluation_results.json` - Full detailed results\n",
    "- `evaluation_results.csv` - Results in CSV format\n",
    "- `evaluation_summary.json` - Summary statistics\n",
    "- `evaluation_distributions.png` - Score distribution plots\n",
    "\n",
    "Download these from the Output section to analyze further."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
